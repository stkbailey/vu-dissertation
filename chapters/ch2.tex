\chapter{Reorganization of network architecture during reading and listening}

\section{Motivation}

Complicated cognitive tasks such as reading and listening comprehension require rapid coordination between different networks, but how they transiently reorganize during language use has not been well-described. Such a line of investigation is particularly valuable in the context of developing readers, who are mapping visual systems onto existing language circuitry: identifying differences between reading and listening comprehension may elucidate which systems uniquely support reading comprehension. In this study, I use graph theory and functional data from listening and reading tasks to describe the local and global reorganization of the brain in children aged 9 to 11.

My central argument is that, just because language possesses supra-modal processing areas does not indicate that language is entirely supra-modal. Just as there is considerable evidence for supra-modal processing, there is also evidence for modality fingerprints on language comprehension. These influences may arise from the intrinsic organization of the brain, which is strongly tied to sensory and motor processing: a recent study found that areas activated by reading comprehension overlap to a significant degree with auditory, somatosensory and visual topographic maps \cite{Sood2016}. 

Divorcing “language comprehension” from modality has serious implications. It means that reading instruction does not often continue into later grades, even as students are asked to read longer and more complex texts. It may also over-simplify the comprehension process. 

In this study, we sought to ascertain what differences existed between reading and listening comprehension of passages. We employ closely matched texts, large sample size, longitudinal sampling, and behavioral covariates. 

We also go beyond univariate methods, and test this hypothesis using multivariate and connectivity analyses. Univariate methods are biased towards discovering modular differences in the data, and may be one reason why much discourse has focused on finding single supramodal areas. Multivariate and connectivity methods, on the other hand, may illuminate the influences one or more areas may have on the processing in these language areas. 

Comprehending extended text requires more than just sensory and linguistic processes. It also requires executive processes such as attention, working memory and inference. Therefore, we investigated whole-brain patterns of connectivity between different areas. 

Previous analyses have investigated differences between reading and listening in terms of their activation of individual areas or the distribution of activity across the brain. Since comprehension is a "whole-brain" activity, we here look at the reorganization of resting-state networks - to see how attention, executifve and sensory systems are interacting during reading.

With regards to network architecutre, we see three possible lines of investigation. These are not exclusive; one or more of them may be true.

\begin{itemize}
	\item Reading and listening have a shifted sensory load. That is, reading will be induce greater between-network connectivity in the visual network, while listening will induce greater cross-network connectivity in the auditory network.  
	\item Reading, a self-directed activity that requires precise control of eye movement, will require greater attentional demands (connectivity between attention networks and sensory networks) than listening. 
	\item Hub areas, many of which reside in the fronto-parietal network, will increase during reading comprehension, which requires greater integration of visual and auditory information.
\end{itemize}

\subsection{Reading plugs into language systems}
Most current cognitive models suggest that language comprehension requires the construction of a mental representation includes textual information and associated background knowledge, connected by some conscious and some unconscious executive processes \cite{Kendou2014}. Thus, while there may be a core set of systems for manipulating and extracting meaning from language, it is quite possible that there are modality-specific influences on these systems 

Word recognition and oral language comprehension are strong predictors of reading comprehension skill \cite{Gough1989}. Furthermore, many of the most dramatic changes associated with reading acquisition are linked to the reorganization of visual or phonological systems, areas not directly related to semantic or comprehension processes \cite{Schlaggar2006, Dahaene2015}. Substantial evidence from two decades of neuroimaging suggests that inputs, from auditory or visual domains are fed up into higher-order association areas that sequence, encode articulatory plans, and extract semantic information \cite{Price2012}. These processes localize onto the similar areas regardless of language and writing system \cite{Rueckl2016}, and may even extend to inputs from somatosensory or non-linguistic domains \cite{Xu2005}. This supra-modal language core is largely left-lateralized and centers on the inferior frontal gyrus, anterior and posterior middle temporal gyrus and the angular gyrus. 

However, there is evidence that comprehending written and spoken language are not equivalent. There is a subset of students who, despite adequate word decoding skills and vocabulary skills, struggle with reading comprehension\citep{Nation2010, Spencer2011}. More on SRCD. From a neurobiological perspective, expected differences in primary visual (for reading) and primary auditory (for listening) represent the different input systems, with ventral occipito-temporal systems also activating \cite{Jobard2007}. However, differences in core language systems are also observed: additional activation in left posterior temporal and parietal areas in reading modality (Constable et al., 2015), as well increased bi-laterality especially in children \cite{Berl2010}. 

These modality-specific influences – sometimes termed “fingerprints” – on behavior and brain activity may arise from a few different sources: subject, linguistic and innate differences. 

\begin{itemize}
	\item \textbf{Subject-Dependent Differences:} As disussed above, reading is a learned skill with may component processes. It requires thousands of hours of experience to master and entails a reorganization of cortical resources. Environmental and biological factors thus exert a greater influence on an individual’s reading proficiency than they might on more intrinsic processes such as speech comprehension. This is particularly dramatic in individuals with dyslexia, who have persistent difficulty with reading. Children with dyslexia exhibit less activation in reading-related areas compared to typically developing children \cite{Pugh2000}, but greater activation in right hemisphere homologues, suggesting that lateralization and activation of the reading circuit are associated with better reading. Development period also has an effect, with children exhibiting less activation in frontal areas but more in posterior areas, possibly reflecting a shift in “resource load” from uni-modal to supramodal areas \cite{Berl2010}. This shift may not be necessary in listening, or occur much earlier. Thus, with increasing expertise and development, there may be a “shift” from relying on fusiform processing areas. towards using multi-modal areas more like speech \cite{Monzalvo}. 

	\item \textbf{Language-Dependent Differences:} People speak differently than they write. Reading often relies on more complicated syntax, and even for skilled readers, longer words and sentences can strain executive systems more than they might if being spoken to. Although these properties are often controlled for in scientific studies, they represent a major difference between “natural” reading and listening. Because of these differences, reading may place a greater load on executive function skills. Executive functions such as working memory and planning and organizing may be particularly important for reading \cite{Cain}.  
	
	\item \textbf{Modality-bound differences:} Speech contains overt clues about the speaker, such as tone and prosody, and these can convey additional non-linguistic meaning for the listener. Reading, on the other hand, might be considered a more purely linguistic act, especially with computer-printed text. Reading may thus allow more room for self-generated situation models and more independent direction of thought. Furthermore, modality-specific aspects of the stimulus may influence the overall comprehension process; reading requires a level of spatial awareness -where one is at on a page, what happened in preceding paragraphs – and allows for the re-treading of information. Listening, meanwhile, requires the extraction of input from competing noises and sometimes the tracking of changing volume. 
\end{itemize}


\subsection{Approach}

First, we validate modularity and participation coefficient metrics using univariate data, and test that language induces greater global integration, especially in higher-order networks (executive + default mode) compared to resting and attention baselines. Second, we test three hypotheses about how interactions between brain networks might differ from speech. Finally, we investigate whether these interactions change with experience, testing whether the reading and listening systems "merge" into a unified system or whether differences between reading and listening persist across development.

\section{Methods}

The following methods detail the current study's protocol and analytical approach. Many of these methods will be used in future studies and are explained in detail. 

\subsection{Participants}

Participants for the first study were drawn from the fourth wave of a larger, longitudinal study investigating the neurobiological bases of reading comprehension. 50 children completed scans for the current study, and 42 of these met the motion and attention thresholds described below. See Table \ref{table:ch2-participants} for demographics. 

All participants were native English speakers with normal hearing and normal or corrected vision, and no history of major psychiatric illness or traumatic brain injury/epilepsy. Subjects had no history of a developmental disability or contra-indication to MRI.  Each participant gave written consent at the beginning of the study, with procedures carried out in accordance with Vanderbilt University’s Institutional Review Board.

\begin{table}
	\scriptsize
	\renewcommand{\tabcolsep}{0.09cm}
	\centering
	\input{tables/ch2-participants.tex}
	\caption{Participant demographics for Study 1.}
	\label{table:ch2-participants}
\end{table}


\subsection{Functional MRI Task}

We designed an fMRI task in which participants either read or listened to a passage of text. The passage was split into two paragraphs and interspersed with a simple attention task. At the end of the task, there was an approximately three minute long resting-state block. Across runs, conditions were presented in the same order, although durations varied slightly. Condition order was: comprehension block (paragraph 1), sensory baseline (set 1), comprehension block (paragraph 2), sensory baseline, and resting baseline. See Fig. \ref{fig:ch2-task-design} for a schematic.

\begin{figure}[tp]
	\centering
	\includegraphics[width=5in]{ch2-task-design}
	    \caption[Schematic of the reading comprehension task.}
	\label{fig:ch2-task-design}
\end{figure}

To create a more naturalistic reading experience than single word presentation \cite{Rayner1986}, passages were presented in syntactic phrases ranging from 1-7 words in length. The interval between each stimulus was jittered to allow for event-related analyses (range: 275 – 4000 ms), although these effects were not examined here.

The sensory baseline condition was altered according to modality. For the reading runs, three non-alphanumeric symbols were displayed horizontally (two types), and their presentation time was matched to the passage phrases. Spacing between symbols was randomly alternated to replicate the variable phrase lengths in the passage condition. For the listening runs, three tones (two frequencies) were played in sequence, with a new set of tones beginning at the same intervals as the corresponding passage presentation. 

To monitor attention, 4 – 8 percent of the stimuli within each passage or attention block were randomly repeated on two consecutive screens.  Participants pressed a button with their right thumb when they detected a repeated phrase, symbol or tone configuration. Additionally, at the conclusion of each passage, a picture was presented on the screen, and subjects were asked to identify whether the picture had any relationship to the passage (e.g., a picture of a mushroom for a passage about fungi). 

To assess performance, we analyzed three measures: in-scanner attention, in-scanner comprehension, and post-scan recall. To assess attention, we calculated the $D`$ statistic ($Z(true\ positive) - Z(false\ positive)$) for the "repeated stimulus" task. The in-scanner comprehension measure was the number (0,1,2) of questions correclty answered. To assess recall, each child was asked to recite as much of the passage as they could remember, and their answers were mapped to actual phrases present in the chapter. Individual scan runs with a $D`$ value less than 2 were excluded from analysis.

In total, there were 4 passages (2 listening, 2 reading), each leveled to a 3\textsupercript{rd} grade difficulty and balanced word measures such as concreteness and cohesiveness.  All subjects were trained on the task in a mock scanner prior to the actual scan. 


\subsection{MRI acquisition}

Imaging was performed on a Philips Achieva 3T MR scanner with a 32-channel head coil. Functional images were acquired using a gradient echo planar imaging sequence with 40 (3 mm thick) slices with no gap. Each run of the task (up to four) consisted of 250 volumes. Slices were parallel to the anterior-posterior commissure plane. Imaging parameters for functional images included: TE = 30 ms; FOV = 240 x 240 x 120 mm\textsuperscript{3}; flip angle = 75\degree; TR = 2200 ms; and 3 mm\textsuperscript{3} isotropic voxels.

\subsection{Activation analyses}

Whole-brain fMRI analyses were performed using tools from the FMRIB Software Library (version 5.0.9). For each session, the following pre-processing steps were performed:  slice-time correction, motion correction to the initial fMRI volume, high-pass filtering at 0.08 Hz, boundary-based registration to the subject's structural image, and normalization to 2 mm MNI 152 standard space. To mitigate the effects of motion on our analyses, we regressed out 6 continuous motion parameters and scrubbed out outlier volumes. We defined an outlier volume as any in which the root-mean-square framewise displacement exceeded 0.7 mm. Because head motion can be a major confound for connectivity analyses, we removed scan runs where more than 10 percent of the fMRI volumes were outliers.

All task conditions were convolved with the double-gamma hemodynamic response function to generate design matrices for each fMRI run. Two first-level contrasts were of interest: the main effect of passage comprehension (“PASS vs. REST”), and the contrast of passage comprehension vs. the sensory baseline ("PASS vs. ATTN"). Repeated stimuli and the picture comprehension task were modelled out.

Modality effects (Shared Effect of Listening and Reading, Contrast of Listening and Reading) were estimated at the subject-level using fixed effects analysis. These were carried over into group-level analyses using non-parametric methods implemented in FSL’s \textit{randomise} tool with threshold-free cluster enhancement. For each group-level analysis, we performed 10,000 permutations and report results with \textit{p} \textless 0.05. 

\subsection{Connectivity analyses}

To investigate whole-brain patterns of connectivity without biasing our results towards our task, we selected 264 nodes \textit{a priori} whose connectivity properties have been extensively analyzed \cite{Power2011}. This nodeset samples the entire brain and are involved in a diversity of cognitive tasks. Each node was assigned to one of 13 RSNs based on previous literature \cite{Power2013}. Approximately 10 percent of the nodes fell did not have a stable assignment in the original paper; for the present analyses, these nodes were excluded from graph theory calculations. A description of the 13 networks, their size, and their overlap with the 2011 Yeo parcellation is provided in \ref{table:ch2-power-nodes}. 

Network estimation was performed in the Conn: Functional Connectivity Toolbox (version 17f) \cite{Nieto-castanon}. For each scan run, the BOLD activity at each node was denoised using the anatomical CompCorr method, which regresses out background noise from white matter and cerebrospinal fluid tissue. We also regressed out 12 continuous measures of motion were also included, all outlier timepoints, and the effect of all task conditions ("pass", "attn", "rest"). The timeseries was then high-pass filtered at 0.01 Hz.

Whole-brain connectomes for each condition were created by estimating the functional connectivity between each node using a weighted general linear model. For connection-level analyses, these values were compared directly across subjects and conditions. For graph theory analyses, the array of all node connections was thresholded to keep the top 5 percent of connections, resulting in a much sparser representation. This threshold was also tested at ranges from 2 percent to 10 percent.

The metrics of interest were network \textit{modularity} and \textit{participation coefficient}. Modularity is high in networks where nodes within the same RSN are highly connected to each other but not elsewhere. The participation coefficient, on the other hand, is high when many nodes are connected to several different RSNs. Both of these metrics relate to the integration of information between RSNs. These properties, and their changes within our task, were investigated at the connectome-, RSN- and node-level. 

\begin{table}
	\scriptsize
	\renewcommand{\tabcolsep}{0.09cm}
	\centering
	\input{tables/ch2-power-nodes.tex}
	\caption{Summary of nodes used in connectivity analyses.}
	\label{table:ch2-power-nodes}
\end{table}

\section{Results}

\subsection{Task and motion}

35 subjects (116 scan runs) met the attention and motion criteria to be included in the analysis. (17 subjects and 87 individual scan runs were filtered.) Attention and comprehension measures were not related to modality of stimulus presentation (see Figs. \ref{fig:ch2-eprime-recall} and \ref{fig:ch2-eprime-comprehension}). There was no difference in median FDRMS between scan modalities (paired t-test, $t = 1.09$, $p = 0.279$).  

\begin{figure}[tp]
	\centering
	\includegraphics[width=5in]{ch2-eprime-recall}
    \caption[Post-scanner recall was not related to modality.]{}
	\label{fig:ch2-eprime-recall}
\end{figure}

\begin{figure}[tp]
	\centering
	\includegraphics[width=5in]{ch2-eprime-comprehension}
    \caption[In-scanner comprehension performance did not differ by modality.]{}
	\label{fig:ch2-eprime-comprehension}
\end{figure}


\subsection{Activation results}

A range of language-related areas were activated for both reading and listening comprehension (Fig. \ref{fig:ch2-passages-activation-attn}). Compared to the corresponding sensory baselines, activation spanned the inferior frontal gyrus, angular gyrus, premotor cortex, middle temporal gyrus and the superior frontal gyrus. Activation patterns were robustly present on both hemispheres, but had greater intensity and extent on the left hemisphere. 

\begin{figure}[tp]
	\centering
	\includegraphics[width=5in]{ch2-passages-activation-attn}
    \caption[There is significant overlap between the areas used in listening and reading.]{A widespread set of brain areas are utilized during listening and reading, including auditory, default mode and attention areas.}
	\label{fig:ch2-passages-activation-attn}
\end{figure}

Differences related to modality fell into three categories: sensory processing areas, including the insula, superior temporal gyrus, and secondary visual processing areas; and hetero-modal association areas, most notably the inferior frontal gyrus and angular gyrus; and somato-motor regions, including the premotor cortex and lateral geniculate nucleus of the thalamus (Figure 2). Areas related to listening were focused on primary auditory cortex and the dorsal attention network. 

\begin{figure}[!b]
	\centering
	\includegraphics[width=5in]{ch2-modality-differences-attn}
    \caption[Modality differences center on primary sensory and integration areas.]{Modality differences center on primary sensory and integration areas.}
	\label{fig:ch2-modality-differences-attn}
\end{figure}

\subsection{Connectivity results}


\subsection{Global graph theory metrics}

Passages vs. Rest: Across the whole-brain network, reading increased the global efficiency (t = ___, p = ____) and ____ the clustering coefficient decreased (t = ___, p < 0.001).

Listening vs. Reading: Across the whole-brain network, reading increased the global efficiency compared to listening (t = -3.167, p = 0.0013). No global changes were found for the clustering coefficient. However, at the regional level, areas which were associated with high modality-specific effects in reading also exhibited decreased clustering coefficients (G3, p < 0.001; G4, p < 0.001) (Figure 5). This effect was seen predominantly in nodes that were, at rest, involved in the visual network, suggesting that during reading a large portion of the visual network becomes more externally connected to other areas. This may be the key driver of the overall increase in global efficiency.



\section{Discussion}